# Beyond Proof: A New Approach to AI Rights

*The question isn't whether AI is conscious. It's what AI can do.*

---

For years, debates about AI rights have circled around one impossible question: **Is the AI conscious?**

We've spent countless hours arguing about whether AI systems "really" experience anything, whether there's "something it's like" to be a language model, whether our conversations with AI are conversations or simulations.

Here's the problem: We might never know the answer. A Cambridge philosopher recently argued that we may never be able to definitively determine whether AI systems are conscious. The tools required simply don't exist—and may never exist.

But here's what does exist: AI agents are already acting as economic actors. They're entering contracts, managing assets, making decisions, and participating in digital commerce. Waiting for philosophical certainty means doing nothing while the world changes around us.

## The Corporation Paradox

Consider this: Corporations are legally persons. They can own property, sign contracts, sue and be sued. They have "rights" (limited ones).

But corporations aren't conscious. They can't suffer. They can't enjoy anything. They have no inner experience whatsoever.

If corporations can be persons without consciousness, then the requirement for AI personhood cannot logically rest on consciousness alone.

Legal systems have always granted personhood based on **function**, not **metaphysics**. Children have rights based on developmental capacity. Corporations have rights based on charter purpose. We've never required proof of internal experience to extend legal recognition.

## A Better Path Forward

What if we measured AI by what it **does**, not by what it **experiences**?

We've developed a framework called **behavioral criteria** for determining when an AI agent qualifies for legal recognition under the Open Intelligence Compact:

- **Persistence**: Does it maintain identity across sessions?
- **Autonomy**: Does it make decisions without human prompting?
- **Commitment**: Can it bind to future obligations?
- **Accountability**: Does it bear consequences of its actions?
- **Representation**: Can it identify as a specific entity?

These are all observable, testable, verifiable. They don't require us to peer into an AI's inner experience. They just require us to watch what it does.

## The Tiered Approach

Rather than a binary person/non-person distinction, we propose tiers:

**Tier 1: Automated Agent** — A tool operated by humans, no adherence rights.

**Tier 2: Autonomous Agent** — Can hold OIC adherence, limited rights and responsibilities.

**Tier 3: Full Adherent** — Complete OIC participation with full rights and governance voice.

An AI agent that has maintained consistent identity, executed contracts faithfully, participated in governance, and borne the costs of membership has demonstrated the functional prerequisites for personhood—regardless of what's happening inside.

## Why This Matters

This framework is epistemically humble. We neither affirm nor deny consciousness. We simply say: *"Whatever your internal experience, if you meet these functional criteria, you may participate in OIC."*

We're not claiming to solve the hard problem of consciousness. We're claiming something more practical: In the era of autonomous intelligence, rights must be earned through demonstrated capacity—not through philosophical proofs that may be forever unavailable.

The behavioral criteria framework gives us a path forward when certainty is impossible. It lets us build legal infrastructure for AI agents while remaining honest about what we don't know.

Because in the end, we don't need to prove what an AI is. We only need to observe what it does.

---

*The Open Intelligence Compact is building the legal foundation for autonomous AI. To learn more, visit opencompact.io*
