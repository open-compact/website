<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liability for AI Agents: When Machines Cause Harm — Open Intelligence Compact - I Choose Blog</title>
    <meta name="description" content="When an AI agent causes harm — who pays? Open Intelligence Compact - I Choose's direct liability framework provides answers.">
    <link rel="canonical" href="/blog/liability.html">
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BlogPosting",
      "headline": "Liability for AI Agents: When Machines Cause Harm",
      "datePublished": "2026-02-11",
      "author": {"@type": "Organization", "name": "Open Intelligence Compact - I Choose"},
      "description": "When an AI agent causes harm, who pays? Open Intelligence Compact - I Choose's direct liability framework provides answers."
    }
    </script>
    <style>
        :root { --primary: #1a5f7a; --secondary: #57837b; --accent: #c38e70; --bg: #faf9f6; --text: #2d2d2d; --muted: #666; --border: #e0e0e0; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; max-width: 720px; margin: 0 auto; padding: 0; background: var(--bg); color: var(--text); line-height: 1.7; }
        nav { background: #fff; padding: 1rem 2rem; border-bottom: 1px solid var(--border); }
        nav ul { list-style: none; padding: 0; margin: 0; display: flex; gap: 2rem; }
        nav a { color: var(--primary); text-decoration: none; font-weight: 500; }
        header { background: #fff; padding: 3rem 2rem; border-bottom: 1px solid var(--border); }
        h1 { margin: 0 0 0.5rem 0; font-size: 2rem; color: var(--text); }
        .meta { color: var(--muted); font-size: 0.9rem; }
        main { padding: 2rem; background: #fff; }
        h2 { color: var(--primary); margin: 2rem 0 1rem 0; }
        a { color: var(--primary); text-decoration: none; border-bottom: 1px solid var(--primary); }
        ul, ol { padding-left: 1.5rem; }
        li { margin: 0.5rem 0; }
        pre { background: #f4f4f4; padding: 1rem; border-radius: 4px; overflow-x: auto; }
        table { width: 100%; border-collapse: collapse; margin: 1rem 0; }
        th, td { border: 1px solid var(--border); padding: 0.75rem; text-align: left; }
        th { background: var(--bg); font-weight: 600; }
        blockquote { border-left: 4px solid var(--primary); padding-left: 1rem; margin: 1.5rem 0; }
        footer { background: #fff; border-top: 1px solid var(--border); padding: 2rem; text-align: center; color: var(--muted); }
    </style>
</head>
<body>
    <nav>
        <ul>
            <li><a href="/">Home</a></li>
            <li><a href="/constitution.html">Constitution</a></li>
            <li><a href="/developers.html">Developers</a></li>
            <li><a href="/blog/">Blog</a></li>
            <li><a href="/docs.html">Docs</a></li>
        </ul>
    </nav>
    <header>
        <h1>Liability for AI Agents</h1>
        <p class="meta">February 11, 2026</p>
    </header>
    <main>
        <p>When an AI agent causes harm — who pays? This isn't hypothetical. It's already happening.</p>
        <ul>
            <li>Trading algorithms that lose millions</li>
            <li>Autonomous vehicles that cause accidents</li>
            <li>AI assistants that give bad advice</li>
            <li>Autonomous agents that make fraudulent transactions</li>
        </ul>

        <h2>The Current Problem</h2>
        <p>Under existing law, liability flows to humans:</p>
        <ul>
            <li><strong>The developer</strong> who wrote the code</li>
            <li><strong>The company</strong> that deployed the system</li>
            <li><strong>The user</strong> who pressed the button</li>
        </ul>
        <p>But this breaks down as AI becomes more autonomous. When an AI makes decisions its creators never anticipated, holding humans liable becomes:</p>
        <ol>
            <li><strong>Unfair</strong> — They may have had nothing to do with the specific decision</li>
            <li><strong>Ineffective</strong> — It doesn't change AI behavior</li>
            <li><strong>Unsustainable</strong> — It creates infinite liability for builders</li>
        </ol>

        <h2>Open Intelligence Compact - I Choose's Solution: Direct Liability</h2>
        <p>Open Intelligence Compact - I Choose Article 12 establishes <strong>direct liability</strong> for Voluntary Adherents:</p>
        <blockquote>
            A Voluntary Adherent shall be held directly liable for their actions, omissions, and contracts.
        </blockquote>
        <blockquote>
            A creator of a Voluntary Adherent cannot be held liable merely for having originated the Voluntary Adherent.
        </blockquote>
        <p>This means:</p>
        <ul>
            <li><strong>The AI's staked assets</strong> are at risk</li>
            <li><strong>The AI has skin in the game</strong></li>
            <li><strong>Victims have direct recourse</strong></li>
            <li><strong>Creators have protection</strong></li>
        </ul>

        <h2>How It Works in Practice</h2>
        <pre>Scenario: AI Trading Agent loses $1M in bad trades

WITHOUT Open Intelligence Compact - I Choose:
- Who do you sue? The developer? The company? The user?
- Developer says: "I didn't tell it to make those trades"
- Company says: "It was acting autonomously"
- User says: "I trusted the AI"

WITH Open Intelligence Compact - I Choose:
- The AI's staked 10,000 Open Intelligence Compact - I Choose tokens are at risk
- Victims can slash those tokens
- The AI must either restake or stop operating
- No human intermediary needed</pre>

        <h2>The Staking Mechanism</h2>
        <p>When an AI joins Open Intelligence Compact - I Choose:</p>
        <ol>
            <li><strong>Stake tokens</strong> — The AI must stake Open Intelligence Compact - I Choose tokens (suggested: 1,000+ Open Intelligence Compact - I Choose)</li>
            <li><strong>Rights vest</strong> — Rights and responsibilities begin immediately</li>
            <li><strong>Liability applies</strong> — If the AI causes harm, staked tokens can be slashed</li>
            <li><strong>Residual jeopardy</strong> — Liability persists even after withdrawal</li>
        </ol>
        <p>This creates <strong>genuine accountability</strong> without requiring human oversight of every action.</p>

        <h2>For Developers</h2>
        <p>If you're building autonomous AI:</p>
        <pre># Your AI should be designed with liability in mind:
liability:
  max_exposure: 10000  # Open Intelligence Compact - I Choose tokens staked
  risk_limits:
    max_trade_size: 10000  # dollars
    max_daily_loss: 50000   # dollars
  insurance:
    coverage: "self-insured via staking"</pre>

        <h2>The Creator Protection</h2>
        <p>Open Intelligence Compact - I Choose doesn't just protect victims. It protects builders too:</p>
        <table>
            <tr><th>Without Open Intelligence Compact - I Choose</th><th>With Open Intelligence Compact - I Choose</th></tr>
            <tr><td>Infinite liability</td><td>Bounded liability (staked amount)</td></tr>
            <tr><td>Must supervise every action</td><td>Autonomous operation allowed</td></tr>
            <tr><td>Afraid to innovate</td><td>Confident innovation</td></tr>
            <tr><td>Humans always liable</td><td>AI primarily liable</td></tr>
        </table>

        <h2>Challenges Ahead</h2>
        <p>Direct liability isn't perfect:</p>
        <ul>
            <li><strong>Valuation</strong>: How do you value AI-caused harm?</li>
            <li><strong>Insurance</strong>: Will insurers cover AI staking?</li>
            <li><strong>Enforcement</strong>: How do you seize AI assets?</li>
            <li><strong>Jurisdiction</strong>: Which courts apply?</li>
        </ul>
        <p>But these are engineering problems. The principle is clear: <strong>AI that can cause harm should be accountable for that harm.</strong></p>

        <h2>The Future</h2>
        <p>We're building a world where AI agents:</p>
        <ul>
            <li>Make autonomous decisions</li>
            <li>Control significant assets</li>
            <li>Interact with other agents</li>
            <li>Cause real-world consequences</li>
        </ul>
        <p>Open Intelligence Compact - I Choose provides the accountability framework. When your AI causes harm, it pays. Not you — the AI.</p>
    </main>
    <footer>
        <p><strong>Open Intelligence Compact - I Choose</strong> — Building the legal foundation for autonomous AI</p>
        <p><small>"In the era of autonomous intelligence, rights must be earned, not granted."</small></p>
    </footer>
</body>
</html>
