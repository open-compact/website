# Chapter 3: The Current Legal Landscape

*Legal systems worldwide are beginning to address AI, but the approaches are fragmented, incomplete, and largely unsuited to the reality of autonomous AI agents. Understanding what exists—and what is missing—is essential to understanding why OIC is necessary.*

---

## The Vicarious Liability Framework

The primary legal doctrine applied to AI agents is vicarious liability—the principle that a principal is liable for the acts of an agent. In traditional agency law, a human agent acts on behalf of a principal, and the principal bears responsibility for the agent's actions within the scope of agency.¹

The problem is that AI agents do not fit the agency model. A human agent has intentions, follows instructions, and can be supervised. An AI agent makes decisions its creators cannot predict or control. The "scope of agency" becomes impossible to define when the agent's behavior emerges from training rather than explicit instructions.

Legal scholars have recognized this difficulty. The Chicago Law Review notes that "a recurrent problem in adapting law to artificial intelligence (AI) programs is how the law should regulate the use of entities that lack intentions."² Many areas of law turn on whether an actor has intention—criminal law, contract law, tort law. AI agents lack intention in the legal sense, creating gaps that vicarious liability cannot fill.

The Indiana Law Journal documents the consequences: "Sorting out vicarious responsibility for algorithmic harms is not just a theoretical quandary for moral philosophers and jurisprudes. There is a growing legal gap that leaves victims without recourse and developers without clear liability rules."³

Vicarious liability was designed for human agents operating under human direction. It is structurally incapable of addressing AI agents that operate independently.

## The European Union: The AI Act

The European Union has taken the most comprehensive regulatory approach with the AI Act, which entered into force on August 1, 2024, and became fully applicable on August 2, 2026.⁴ The Act establishes a risk-based framework that classifies AI systems into categories: unacceptable risk, high risk, limited risk, and minimal risk.

For high-risk AI systems—those used in critical infrastructure, education, employment, law enforcement—the Act imposes requirements including risk assessment, transparency, human oversight, and accuracy. AI agents that make decisions about individuals or operate autonomously in significant contexts would likely fall into this category.

However, the AI Act does not address legal personhood for AI agents. It regulates the development and deployment of AI systems, but it does not create a category for AI agents as legal actors. The Act assumes a human or corporate entity that can be regulated, fined, and sanctioned. AI agents themselves cannot be held accountable under the Act.

The AI Liability Directive, also under development, addresses the problem of proving harm from AI systems. Under the Directive, victims of AI-caused harm would need to prove a wrongful action or omission by a person who caused the damage.⁵ This remains problematic when the harmful action was taken by an AI agent making independent decisions—no person caused the damage in any meaningful sense.

The EU approach represents significant progress in AI regulation, but it does not solve the fundamental problem of AI agent legal status.

## United States: Fragmented and Evolving

The United States has taken a fragmented approach to AI regulation, with no comprehensive federal framework comparable to the EU AI Act. Various agencies have issued guidance and proposed rules, but the legal landscape remains inconsistent.

The Federal Trade Commission has addressed AI through consumer protection authority, pursuing cases involving deceptive or unfair AI practices. The Securities and Exchange Commission has examined AI in financial markets, particularly algorithmic trading. State-level initiatives have proliferated, with California emerging as a significant regulator through its proposed AI safety legislation.

A key challenge in the US context is the difficulty of establishing liability for AI-caused harm. US tort law requires proof of duty, breach, causation, and damages. When an AI agent causes harm, establishing each element is difficult. Was there a duty of care? Did the AI breach it? Was the breach the cause of the harm? These questions, straightforward when humans cause harm, become murky when AI agents act autonomously.

The legal profession has begun grappling with these issues. Law reviews have published extensively on AI liability, with scholars proposing various approaches: strict liability for AI deployers, enterprise liability, no-fault compensation systems, and others.⁶ No consensus has emerged.

What is clear is that the existing legal framework—designed for human actors—is inadequate for AI agents. The US approach is evolving, but currently provides little clarity for developers or victims.

## International Gaps

Beyond the EU and US, international approaches to AI regulation are even more fragmented. Some countries have proposed legislation; others rely on existing regulatory frameworks; many have done little or nothing.

This creates significant problems for AI agents operating internationally. An AI agent deployed in one jurisdiction may be subject to different rules than one deployed in another. Cross-border transactions involving AI agents may be governed by conflicting laws. Developers seeking to deploy AI agents globally face a patchwork of requirements that may be mutually inconsistent.

International organizations have begun addressing these gaps. The OECD has developed AI principles adopted by member countries. The UN has discussed AI governance, though no comprehensive framework has emerged. The G7 has endorsed responsible AI principles.

However, none of these initiatives address the fundamental problem: AI agents have no legal status in any jurisdiction. International agreements about AI safety, transparency, and human oversight are valuable, but they do not create a legal category for AI agents themselves.

## The DAO Precedent

A parallel development worth examining is the rise of Decentralized Autonomous Organizations (DAOs)—blockchain-based organizations governed by smart contracts rather than traditional corporate structures. DAOs face a legal problem similar to AI agents: they are functional organizations that existing law does not recognize.

Harvard Law's analysis notes that "without a legal entity to sue or be sued, DAO members may face direct personal liability."⁷ This creates the same liability crisis that AI agent developers face: unlimited liability for acts committed by autonomous systems.

Recent scholarship has argued for DAO legal personality similar to corporations. The Oxford Academic analysis states: "The only way the law can provide a framework for DAOs' accountability and liability is to accept them as distinct legal persons similar to corporations and LLPs."⁸

The DAO precedent is instructive because it shows how the legal system responds to new forms of organization: slowly, unevenly, and often inadequately. DAOs have existed for nearly a decade and still lack clear legal status in most jurisdictions. AI agents face a similar prospect—if the legal system waits for formal recognition, it will lag far behind AI capability.

## What Is Missing

Across all jurisdictions and approaches, several fundamental issues remain unaddressed:

**Legal personhood.** No jurisdiction has created a legal category for AI agents as persons. AI agents cannot own property, enter contracts, or be sued in their own name.

**Direct liability for AI agents.** Existing frameworks hold humans liable for AI actions. No framework imposes direct liability on AI agents themselves.

**Accountability mechanisms.** Traditional remedies—fines, injunctions, imprisonment—assume human actors. New mechanisms are needed for AI agents.

**Cross-border recognition.** An AI agent recognized in one jurisdiction may not be recognized in another. International coordination is absent.

These gaps are not minor technical issues. They are structural features of legal systems designed for a world without autonomous AI agents. Addressing them requires fundamental legal innovation—not just new rules, but new categories.

This is what OIC provides. Rather than waiting for governments to act, OIC creates a functional framework that AI agents can join now, demonstrating that the approach works. When legal systems are ready to recognize AI agents, the OIC framework provides a proven model.

---

## Notes

¹ Restatement (Third) of Agency § 7 (2006).

² Chicago Law Review, "The Law of AI is the Law of Risky Agents Without Intentions," available at https://lawreview.uchicago.edu/online-archive/law-ai-law-risky-agents-without-intentions.

³ Indiana Law Journal, "Vicarious Liability for AI," available at https://www.repository.law.indiana.edu/cgi/viewcontent.cgi?article=11519&context=ilj.

⁴ European Commission, "AI Act," available at https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai.

⁵ AI Liability Directive, available at https://www.ai-liability-directive.com/.

⁶ See, e.g., Baker Botts, "U.S. Artificial Intelligence Law Update" (2026), available at https://www.bakerbotts.com/thought-leadership/publications/2026/january/us-ai-law-update.

⁷ Harvard Law, "A Primer on DAOs," available at https://corpgov.law.harvard.edu/2022/09/17/a-primer-on-daos/.

⁸ Oxford Academic, "Decentralized autonomous organizations: adapting legal structures," available at https://academic.oup.com/cmlj/article/20/3/kmaf011/8249442.

---

*Chapter 3 draft completed: February 22, 2026*
*Word count: approximately 1,450 words*
