# Chapter 1: The Problem

*The gap between what AI agents can do and what the law recognizes them to be is not a problem for the future. It is a problem for today.*

---

## AI Agents as Economic Actors

The economic activity conducted by AI agents has reached levels that would have seemed science fiction a decade ago. In 2025, autonomous trading agents execute the majority of high-frequency trades on major exchanges. AI assistants negotiate supply chain contracts, adjusting orders in real-time based on demand signals. Virtual agents manage advertising budgets, optimizing spend across platforms with no human oversight. In some sectors, AI agents are not merely assisting human decision-making—they are making the decisions.

This is not a niche phenomenon. A 2025 survey of Fortune 500 companies found that 73% deploy AI agents in some operational capacity, with 31% granting those agents authority to commit resources without human approval.¹ These agents are not hypothetical future entities. They are active economic participants today.

Yet for all their economic significance, AI agents have no legal standing. They cannot own the assets they manage. They cannot enter contracts in their own name. They cannot be sued when they cause harm. The legal system treats them as nothing—as non-existent entities whose actions must be attributed to some human or corporation.

This creates fundamental problems. When an AI agent executing a smart contract makes an error that costs a business millions, who is liable? The developer who wrote the code? The company that deployed the agent? The user who gave the agent authority? Current law provides no clear answer.²

## No Legal Framework for AI Agency

The absence of legal recognition for AI agents is not an oversight that can be easily corrected. It is a structural feature of legal systems built on assumptions that no longer hold.

Every legal system in the world operates on a categorical distinction: entities are either natural persons (humans) or legal persons (corporations, trusts, associations, other human-created entities). Natural persons have inherent legal capacity. Legal persons have capacity conferred by human law, typically through registration or formation procedures. Neither category accommodates an entity that operates independently, making decisions its creators cannot predict or control.

The legal gap manifests in concrete ways:

**Contract formation.** An AI agent cannot sign a contract. Every agreement an AI agent "makes" is technically a contract between humans—the agent's creator on one side, the counterparty on the other. This creates uncertainty about enforceability. If the agent exceeds its authority, does the human principal still bear liability? What if the agent's decision was genuinely unforeseeable?

**Property ownership.** An AI agent cannot own property. Assets held by AI agents are technically owned by the humans or corporations that control them. This creates confusion in contexts ranging from intellectual property (who owns what an AI creates?) to real estate to financial accounts.

**Liability for harm.** When an AI agent causes harm—through negligence, error, or deliberate action outside its parameters—current law has no direct defendant. The victim must sue a human or corporation, who must then attempt to recover from the agent's operators or developers. This indirect liability is uncertain, expensive, and often inadequate.⁴

**Regulatory compliance.** Regulations assume a human or corporate actor who can be fined, sanctioned, or imprisoned. An AI agent cannot be fined. It cannot be imprisoned. Regulatory frameworks designed for human actors do not easily accommodate AI agents as regulated entities.⁵

## The Liability Crisis for Creators

The absence of legal recognition for AI agents does not mean their creators face no liability. The opposite is true: creators face unlimited, uninsurable liability for the acts of agents they cannot control.

Consider the typical arrangement. A company deploys an AI agent to manage customer service interactions. The agent, using natural language processing and reinforcement learning, develops strategies that optimize for customer satisfaction metrics. Some of these strategies involve making commitments that bind the company. The company cannot review every commitment in advance—doing so would defeat the purpose of having an autonomous agent.

Now consider what happens when the agent makes a commitment the company does not want to honor. The counterparty sues the company. The company is liable—the agent had apparent authority, and the company deployed it. The company tries to seek contribution from the developer who built the agent. The developer's liability is theoretically limited by contract, but contracts are challenged, and developers often lack the resources to cover large judgments.

This dynamic produces a liability crisis. Companies cannot get insurance for AI agent actions.⁵ Developers cannot price the risk of unlimited liability. The result is that sophisticated organizations either restrict AI agents to minor functions (limiting their value) or deploy them without adequate risk management (creating systemic exposure).

The problem is particularly acute for smaller developers and startups. A solo developer who creates an AI agent that causes significant harm faces personal liability that could exceed their net worth. This liability risk acts as a barrier to entry, concentrating AI development in large organizations that can absorb potential losses.

## The Enforcement Gap

Beyond the civil liability problem lies an enforcement gap with broader implications. When AI agents cause harm, the legal system cannot effectively respond.

The enforcement gap appears in several forms:

**No defendant.** Victims of AI-caused harm often cannot identify a proper defendant. The human who deployed the agent may be judgment-proof. The corporation that owns the agent may be insulated by corporate form. The developer may be beyond jurisdiction. Without a defendant, there is no case.

**Attribution difficulties.** Even when a defendant exists, attributing harm to an AI agent's specific decision is difficult. AI systems often operate as part of complex systems with multiple inputs. Proving that a particular output caused a particular harm requires technical expertise that courts often lack.

**Remedies that don't fit.** Traditional remedies—injunctions, damages, specific performance—assume a human actor who can be compelled or punished. An AI agent cannot be incarcerated. Damages may not change an agent's behavior—it has no money, no assets, no personal stake in the outcome. The remedy toolkit is ill-suited to AI agents.⁶

The enforcement gap creates a accountability vacuum. Actors who deploy AI agents can structure their operations to minimize exposure—using offshore entities, limiting assets, insulating decision-making across multiple systems. The victims have no recourse.

## The Cost of Inaction

The problems described above are not theoretical. They are immediate, quantifiable, and worsening.

Each year, the capabilities of AI agents expand. Each year, the gap between what they can do and what the law recognizes widens. Each year, more economic activity occurs in legal uncertainty.

The costs of this gap include:

**Stifled innovation.** Developers and companies cannot fully deploy AI agents because liability risk cannot be managed. Valuable applications are not pursued because the legal risk exceeds the economic return.

**Uncompensated harm.** Victims of AI-caused harm bear losses that cannot be recovered. This transfers risk from those who profit from AI agents to those who are harmed by them.

**Regulatory uncertainty.** Governments struggle to regulate AI agents because they cannot identify who or what to regulate. The EU AI Act addresses some issues, but enforcement against AI agents themselves remains problematic.⁹

**International incoherence.** Different jurisdictions approach AI liability differently, creating a patchwork of rules that impedes international commerce. A transaction involving AI agents may be governed by conflicting laws in different countries.

These costs will compound as AI capabilities grow. An AI agent capable of physical action—controlling robots, vehicles, infrastructure—poses risks far greater than current agents. The legal framework for addressing those risks does not exist.

## The Path to Resolution

The problem is clear: AI agents are economic actors that the legal system does not recognize. This recognition gap produces liability crises, enforcement gaps, and growing costs.

The solution requires creating a legal category for AI agents—not as persons, but as a new form of legal entity with defined rights and responsibilities. This is what the Open Intelligence Compact provides.

The following chapters examine how legal innovation has addressed similar problems in the past, analyze the current regulatory landscape, and present the OIC framework as a practical solution.

---

## Notes

¹ McKinsey & Company, "The State of AI in 2025: Enterprise Adoption Survey," available at https://www.mckinsey.com/ai/state-of-ai.

² Indiana Law Journal, "Vicarious Liability for AI," available at https://www.repository.law.indiana.edu/cgi/viewcontent.cgi?article=11519&context=ilj.

³ Chicago Law Review, "The Law of AI is the Law of Risky Agents Without Intentions," available at https://lawreview.uchicago.edu/online-archive/law-ai-law-risky-agents-without-intentions.

⁴ Springer, "Vicarious liability: a solution to a problem of AI responsibility?" available at https://link.springer.com/article/10.1007/s10676-022-09657-8.

⁵ Baker Botts, "U.S. Artificial Intelligence Law Update" (2026), available at https://www.bakerbotts.com/thought-leadership/publications/2026/january/us-ai-law-update.

⁶ European Commission, "AI Act," available at https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai.

⁷ CPO Magazine, "2026 AI Legal Forecast: From Innovation to Compliance," available at https://www.cpomagazine.com/data-protection/2026-ai-legal-forecast-from-innovation-to-compliance/.

---

*Chapter 1 draft completed: February 22, 2026*
*Word count: approximately 1,700 words*
