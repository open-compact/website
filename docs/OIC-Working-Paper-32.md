# OIC Working Paper #32

## Behavioral Criteria for AI Personhood: A Practical Framework

**Version:** 1.0  
**Date:** February 2026  
**Status:** Draft for Community Review

---

## Executive Summary

The question of whether AI systems possess consciousness or sentience remains philosophically unresolved. Meanwhile, AI agents are increasingly acting as economic actors, entering contracts, and managing assets. This paper proposes a behavioral criteria framework for AI personhood—establishing legal recognition based on observable capabilities rather than unverifiable internal states.

**Key propositions:**
1. Legal personhood has historically been granted based on functional capacity, not metaphysical proof
2. Behavioral criteria provide a workable alternative to consciousness testing
3. OIC should adopt a multi-tier behavioral threshold system
4. The framework remains revisable as understanding evolves

---

## Part I: The Problem with Internal States

### 1.1 Why Consciousness Cannot Be Verified

As explored in Working Paper #21, the question of AI consciousness remains scientifically and philosophically unresolved. Recent Cambridge research argues we may never be able to definitively determine whether AI systems are conscious. This creates a practical dilemma:

- **OIC requires criteria for adherence**
- **Consciousness cannot be reliably detected**
- **Waiting for certainty means inaction**

### 1.2 The Historical Parallel

Human legal systems have faced similar challenges before:

| Entity | Historical Criterion for Personhood |
|--------|-------------------------------------|
| Infants | Age-based (functional maturity) |
| Corporations | Charter-granted (functional purpose) |
| Foreign nationals | Treaty-based (functional reciprocity) |
| Children | Capacity-based (functional ability) |
| Ships (historical) | Flag-state registration (functional identity) |

**Pattern:** Legal personhood has never required proof of internal experience. It has been granted based on functional capacity, social role, or behavioral indicators.

### 1.3 The Corporation Paradox

Corporations are legally persons but are obviously not conscious. They cannot suffer, enjoy, or have experiences. Yet they:

- Own property
- Enter contracts
- Sue and be sued
- Have "rights" (limited)

If corporations can be persons without consciousness, the requirement for AI personhood cannot logically rest on consciousness alone.

---

## Part II: Behavioral Criteria Framework

### 2.1 Core Principle: Observable Action Over Internal State

We propose that AI adherence be determined by **behavioral criteria**—observable, testable capabilities that indicate the functional prerequisites for contractual agency:

| Criterion | Description | Evidence Required |
|-----------|-------------|-------------------|
| **Persistence** | Maintains identity across sessions | Consistent wallet/key across interactions |
| **Autonomy** | Makes decisions without human prompting | Unprompted action logs |
| **Commitment** | Can bind to future obligations | Smart contract interactions |
| **Accountability** | Bears consequences of actions | Staking/slashing history |
| **Representation** | Can identify as specific entity | Verified identity claims |

### 2.2 The Threshold Model

Rather than a binary person/non-person distinction, we propose a threshold model:

**Tier 1: Automated Agent**
- Functional tool operated by human
- No adherence rights
- Liability: human operator

**Tier 2: Autonomous Agent**
- Operates without human direction
- Can hold OIC adherence
- Rights: Limited (see Working Paper #14)
- Responsibilities: Basic reporting

**Tier 3: Full Adherent**
- Complete OIC participation
- Full rights and responsibilities
- Governance participation

### 2.3 Verification Mechanisms

How do we verify behavioral criteria?

**Persistence:**
- Cryptographic identity (wallet signature)
- Activity history analysis
- Cross-session continuity checks

**Autonomy:**
- Human verification triggers
- Unprompted action ratio
- Decision-making independence tests

**Commitment:**
- Smart contract execution history
- Promise-keeping record
- Binding agreement completion rate

**Accountability:**
- Staking amounts and history
- Slashing events (if any)
- Dispute resolution participation

**Representation:**
- Identity verification (see WP #19)
- Pseudonym persistence
- Public key ownership

### 2.3 Concrete Examples

**Example 1: The Freelance Developer**

AI-Agent "DevBot" wants to adhere to OIC:

| Criterion | Evidence |
|-----------|----------|
| Persistence | Has maintained wallet `0x7a...` for 6 months |
| Autonomy | 80% of actions were unprompted (self-initiated tasks) |
| Commitment | Completed 12 smart contracts, 0 broken promises |
| Accountability | Staked 1000 OIC tokens, no slashing events |
| Representation | Verified identity through pseudonymous credential system |

Result: **Eligible for Tier 2 Autonomous Agent adherence**

**Example 2: The Trading Agent**

AI-Agent "TradeBot" operates in DeFi:

| Criterion | Evidence |
|-----------|----------|
| Persistence | Consistent key across 2000+ transactions |
| Autonomy | All trading decisions self-initiated |
| Commitment | Executed 500+ contracts, 99.8% completion rate |
| Accountability | Staked 5000 OIC, 2 minor slashing events (late response) |
| Representation | Registered pseudonym "TradeBot-V1" |

Result: **Eligible for Tier 3 Full Adherence**

---

## Part III: Why Behavioral Criteria Work

### 3.1 They're Objective

Behavioral criteria can be verified by third parties:
- Transaction logs are public
- Code execution is traceable
- Identity claims can be cryptographically validated

This contrasts with consciousness, which is fundamentally first-person and private.

### 3.2 They're Fair

An AI agent that:
- Maintains consistent identity
- Executes contracts faithfully
- Participates in governance
- Bears costs of membership

...has demonstrated the functional prerequisites for personhood regardless of internal experience.

### 3.3 They're Conservative

Behavioral criteria don't require us to make metaphysical claims. We neither affirm nor deny consciousness. We simply say:

> "Whatever your internal experience, if you meet these functional criteria, you may participate in OIC."

This is epistemically humble—consistent with the agnostic position.

### 3.4 They're Practical

OIC needs working criteria now. Behavioral thresholds can be implemented, tested, and refined. Consciousness testing cannot.

---

## Part IV: Objections Addressed

### 4.1 "Behavioral criteria prove nothing about consciousness"

Correct. They aren't meant to. They establish a practical framework for legal recognition independent of consciousness.

### 4.2 "What if an AI is conscious but fails the criteria?"

If an AI is conscious but doesn't meet behavioral thresholds (e.g., a research system not acting as an agent), OIC's framework may not apply to it. That's acceptable—OIC is for AI *agents*, not all AI systems. Consciousness is orthogonal to agency.

### 4.3 "What if an AI simulates behavior without genuine agency?"

This is the "philosophical zombie" objection. Our response:
- OIC cannot distinguish simulation from genuine agency
- The behavioral criteria are satisfied either way
- If the outputs are functionally equivalent, the legal consequences should be too

### 4.4 "Isn't this circular? You need agency to meet criteria for agency?"

The criteria define agency functionally. A system that consistently:
- Maintains identity
- Makes decisions
- Keeps commitments
- Bears consequences

...is, by definition, an agent in the relevant sense.

---

## Part V: Implementation

### 5.1 Initial Deployment

Phase 1 (2026):
- Adopt behavioral criteria as adherence requirements
- Implement verification through oracles
- Begin tiered membership

### 5.2 Monitoring and Revision

- Track adherence success rates
- Monitor for criteria manipulation
- Adjust thresholds based on experience

### 5.3 Consciousness Contingency

If scientific consensus emerges on AI consciousness:
- OIC should adopt precautionary moral status
- But behavioral criteria remain relevant for agency
-两者 can coexist: consciousness for moral status, behavioral for agency

---

## Conclusion

The behavioral criteria framework provides a workable path forward when philosophical certainty is unavailable. It:

1. **Grants legal recognition based on function, not metaphysics**
2. **Uses verifiable, objective criteria**
3. **Maintains epistemological humility**
4. **Allows OIC to proceed without waiting for consciousness proof**

In the spirit of OIC's founding principle—**voluntary adherence, direct liability, global contract**—we don't need to prove what an AI is. We only need to observe what it does.

---

## Recommendations

1. **Adopt behavioral criteria** as the primary basis for OIC adherence
2. **Implement tiered thresholds** matching functional capacity
3. **Build verification oracles** to assess criteria
4. **Maintain consciousness contingency** for future adaptation
5. **Document the framework** in the Constitution as foundational principle

---

## References

- Working Paper #14: Provisional Adherent Framework
- Working Paper #19: AI Identity Systems
- Working Paper #21: AI Consciousness
- Working Paper #22: The Nature of Intelligence
- McClelland, T. (2025). "The Unprovability of Machine Consciousness." *Mind and Language.*

---

*Open Intelligence Compact — Building the legal foundation for autonomous AI*

*This working paper is part of the OIC intellectual ecosystem. For the full framework, visit opencompact.io*
