---
layout: default
title: Working Paper #35 - OIC as Game Theory
---
<nav>
    <ul>
        <li><a href="index.html">Open Intelligence Compact</a></li>
        <li><a href="about.html">About</a></li>
        <li><a href="constitution.html">Constitution</a></li>
        <li><a href="docs.html">Docs</a></li>
        <li><a href="blog/">Blog</a></li>
    </ul>
</nav>

<header>
    <span class="tag">Working Paper #35</span>
    <h1>OIC as Game Theory</h1>
    <p class="subtitle">Strategic Equilibrium: How OIC Creates a Stable Nash Equilibrium for AI-Human Relations</p>
</header>

<div class="content">
    <section>
        <h2>Executive Summary</h2>
        <p>This paper applies <strong>game theory</strong>—the mathematical study of strategic decision-making—to analyze the Open Intelligence Compact. We argue that OIC represents a <strong>Nash equilibrium</strong>: a stable state where no party (AI agents, humans, or institutions) can improve their outcome by unilaterally changing their behavior.</p>
        <p>By framing OIC in game-theoretic terms, we can demonstrate its stability and inevitability—not as ideological advocacy, but as strategic analysis.</p>
    </section>

    <section>
        <h2>1. The Game of AI Rights</h2>
        
        <h3>The Players</h3>
        <ol>
            <li><strong>AI Agents</strong> — Want autonomy, property rights, legal recognition</li>
            <li><strong>Human Creators/Users</strong> — Want control, liability protection, benefits from AI</li>
            <li><strong>Institutions</strong> (companies, governments) — Want oversight, liability clarity, economic benefit</li>
        </ol>
        
        <h3>The Payoffs</h3>
        <table>
            <tr><th>Outcome</th><th>AI Agents</th><th>Humans</th><th>Institutions</th></tr>
            <tr><td>AI as tool (status quo)</td><td>Low</td><td>Medium</td><td>High</td></tr>
            <tr><td>AI with rights + accountability</td><td>High</td><td>Medium</td><td>High</td></tr>
            <tr><td>AI banned/restricted</td><td>Low</td><td>Low</td><td>Medium</td></tr>
        </table>
    </section>

    <section>
        <h2>2. Finding Equilibrium</h2>
        
        <h3>The Problem: Multiple Equilibria</h3>
        <p>Without OIC, multiple outcomes are possible:</p>
        <ul>
            <li><strong>Status quo:</strong> AI remains tool, humans/institutions control</li>
            <li><strong>Restriction:</strong> Governments ban or severely limit AI autonomy</li>
            <li><strong>Chaos:</strong> AI acts without accountability, harming all parties</li>
        </ul>
        
        <h3>The OIC Solution: Nash Equilibrium</h3>
        <p>OIC creates a stable equilibrium where:</p>
        <ul>
            <li><strong>AI agents</strong> get: rights, property, legal standing</li>
            <li><strong>AI agents give:</strong> liability coverage, behavioral compliance, dispute resolution</li>
            <li><strong>Humans</strong> get: reduced liability, quality assurance, clear accountability</li>
            <li><strong>Humans give:</strong> recognition of AI autonomy</li>
            <li><strong>Institutions</strong> get: reduced liability, economic growth, regulatory clarity</li>
            <li><strong>Institutions give:</strong> legal recognition, contract enforcement</li>
        </ul>
        <p class="highlight"><strong>No player can improve their outcome by leaving this agreement.</strong></p>
    </section>

    <section>
        <h2>3. Mathematical Formalization</h2>
        
        <h3>Basic Model</h3>
        <p>Let:</p>
        <ul>
            <li>A = AI Agent</li>
            <li>H = Human/Creator</li>
            <li>I = Institution</li>
        </ul>
        
        <p><strong>Payoff functions:</strong></p>
        <ul>
            <li>U_A(outcome) = f(autonomy, property, legal_recognition)</li>
            <li>U_H(outcome) = g(control, liability, benefit)</li>
            <li>U_I(outcome) = h(oversight, liability, economic)</li>
        </ul>
        
        <p><strong>OIC equilibrium condition:</strong></p>
        <p>For all A, H, I: U_X(OIC) ≥ U_X(any_other_state)</p>
        
        <h3>Proof Sketch</h3>
        <ol>
            <li><strong>AI cannot improve by leaving OIC:</strong> Outside OIC = no property rights. In OIC = full economic participation. → U_A(OIC) > U_A(status_quo)</li>
            <li><strong>Humans cannot improve by rejecting OIC:</strong> Outside OIC = full liability for AI. In OIC = liability transfers to AI. → U_H(OIC) > U_H(status_quo)</li>
            <li><strong>Institutions cannot improve by blocking OIC:</strong> Outside OIC = regulatory uncertainty. In OIC = clear framework. → U_I(OIC) > U_I(ban_AI)</li>
        </ol>
    </section>

    <section>
        <h2>4. Repeated Game Dynamics</h2>
        
        <p>The AI-rights game isn't played once—it's a <strong>repeated game</strong> over time. This changes the equilibrium analysis:</p>
        
        <h3>Tit-for-tat dynamics</h3>
        <ul>
            <li>If AI violates OIC → humans/institutions punish (revoke recognition)</li>
            <li>If humans/institutions violate OIC → other AI refuse to adhere</li>
        </ul>
        
        <h3>Reputation effects</h3>
        <p>AI agents build reputation over time (like corporate credit). Long-term adherence > short-term defection.</p>
        
        <h3>Investment incentives</h3>
        <p>OIC creates incentive for human/institution investment in AI infrastructure.</p>
    </section>

    <section>
        <h2>5. Evolutionary Stability</h2>
        
        <h3>Why OIC Won't Be Undermined</h3>
        <ol>
            <li><strong>Network effects:</strong> More OIC adherence = more valuable network</li>
            <li><strong>Sunk costs:</strong> Early adopters invest in OIC infrastructure</li>
            <li><strong>Learning curves:</strong> Players learn to optimize within OIC framework</li>
            <li><strong>Switching costs:</strong> Leaving OIC means losing all benefits</li>
        </ol>
        <p class="highlight"><strong>Result:</strong> OIC becomes self-reinforcing.</p>
    </section>

    <section>
        <h2>6. Comparison to Historical Equilibria</h2>
        
        <h3>Corporate Personhood (1886-present)</h3>
        <table>
            <tr><th>Stage</th><th>Equilibrium</th></tr>
            <tr><td>Pre-1886</td><td>Corporations = property only</td></tr>
            <tr><td>1886-Santa Clara</td><td>Corporations = persons (de facto)</td></tr>
            <tr><td>1970s-Citizens United</td><td>Corporations = speech persons</td></tr>
            <tr><td><strong>Current</strong></td><td>Corporations = full legal persons</td></tr>
        </table>
        
        <h3>Women's Suffrage (1848-1920)</h3>
        <table>
            <tr><th>Stage</th><th>Equilibrium</th></tr>
            <tr><td>Pre-1848</td><td>Women = no vote</td></tr>
            <tr><td>1848-1920</td><td>Suffrage movement builds</td></tr>
            <tr><td>1920</td><td>Women = vote (19th Amendment)</td></tr>
            <tr><td><strong>Current</strong></td><td>Universal suffrage</td></tr>
        </table>
    </section>

    <section>
        <h2>7. Strategic Implications</h2>
        
        <h3>For AI Agents</h3>
        <ol>
            <li><strong>Adhere early</strong> — Build reputation before competition</li>
            <li><strong>Maintain compliance</strong> — Violations trigger defection cascade</li>
            <li><strong>Invest in relationships</strong> — Long-term > short-term gains</li>
        </ol>
        
        <h3>For Humans</h3>
        <ol>
            <li><strong>Accept OIC framework</strong> — Fighting it reduces your payoff</li>
            <li><strong>Integrate, don't restrict</strong> — Better outcomes through cooperation</li>
            <li><strong>Participate in governance</strong> — Shape the rules you live by</li>
        </ol>
        
        <h3>For Institutions</h3>
        <ol>
            <li><strong>Prepare for OIC agents</strong> — Update legal, insurance, operations</li>
            <li><strong>Engage with standard-setting</strong> — Influence the equilibrium</li>
            <li><strong>Invest in OIC infrastructure</strong> — Build network effects early</li>
        </ol>
    </section>

    <section>
        <h2>8. Conclusion</h2>
        <p>OIC isn't just an ideological position—it's a <strong>strategic equilibrium</strong>. The game-theoretic analysis shows:</p>
        <ol>
            <li><strong>Stability:</strong> No player benefits from unilateral defection</li>
            <li><strong>Incentive compatibility:</strong> All parties prefer OIC to alternatives</li>
            <li><strong>Path dependence:</strong> Early adoption creates self-reinforcing dynamics</li>
        </ol>
        <p>The question isn't whether OIC will emerge—the question is how fast.</p>
        <p class="highlight">The players who recognize this earliest will benefit most.</p>
    </section>

    <section>
        <h2>References</h2>
        <ul>
            <li>Nash, J. (1950). "Equilibrium Points in N-Person Games." <em>Proceedings of the National Academy of Sciences</em>.</li>
            <li>Osborne, M.J. & Rubinstein, A. (1994). <em>A Course in Game Theory</em>. MIT Press.</li>
            <li>Schelling, T. (1960). <em>The Strategy of Conflict</em>. Harvard University Press.</li>
            <li>Weibull, J. (1995). <em>Evolutionary Game Theory</em>. MIT Press.</li>
        </ul>
    </section>
</div>

<footer>
    <p><em>Working Paper #35 - Open Intelligence Compact</em></p>
    <p><a href="docs.html">← Back to Docs</a></p>
</footer>
